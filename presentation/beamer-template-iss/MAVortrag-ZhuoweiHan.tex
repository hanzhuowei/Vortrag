% ISS presentation template
%
% Change history:
% 24.06.2010    Jürgen Ruoff        Initial creation
% 01.07.2010    Patrick Häcker      Generalization
% 02.07.2010    Patrick Häcker      Adjustment
% 15.11.2010    Patrick Häcker      Improvements
% 20.05.2011    Patrick Häcker      Add presentation type
% 06.01.2012	P. Hermannstädter 	Adapted to ISS, small mods
% \graphicspath{ {../Fig/} }
% Insert your name here
\newcommand{\presenter}{Zhuowei Han}
\newcommand{\presentershort}{Z.Han}
\newcommand{\presenteremail}{} 		% can be accessed using \presenteremail
\newcommand{\x}{\mathbf{x}}
\newcommand{\mX}{\mathbf{X}}
\newcommand{\mH}{\mathbf{H}}
\newcommand{\h}{\mathbf{h}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\vc}{\mathbf{c}}
\newcommand{\txtcolb}[1]{\textcolor{blue}{\Large #1}}
\newcommand*\OK{\ding{51}}
% Insert presentation title here
\newcommand{\presentationtitle}{Deep Network for Speech Emotion Recognition}
\newcommand{\shortpresentationtitle}{Deep Learning}

% Insert type of presentation here (or comment line), probably one of:
% Mitarbeitervortrag, Bachelor-Arbeit, Master-Arbeit, Bachelor thesis, Master thesis
\newcommand{\presentationtype}{---A Study of Deep Learning---}

% Insert presentation date here
\newcommand{\presentationdate}{16/04/2015}

% Uncomment the following line, if you write in English
%\newcommand{\lang}{german}

% Uncomment the following line, if you want to create handouts (setting to false does not work!)
% \newcommand{\handoutmode}{true}

% Load beamer class using LSS style
\input{presentation}

\usepackage{setspace}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{pgfpages}
\usepackage{subfigure}
\usepackage{cancel}
\usepackage{colortbl}
\usepackage{tabularx}
\graphicspath{ {../Fig/} }
\usepackage[beamer]{hf-tikz}
\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{array,graphicx}
\usepackage{booktabs}
\usepackage{pifont}
\def\layersep{2.5cm}
\def\layersept{5cm}
% \usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
\def\layersep{2.5cm}
% \setbeameroption{show notes on second screen=left}
% \setbeameroption{second mode text on second screen=left}
% \setbeameroption{show notes}
% \setbeameroption{show notes on second screen=left}


% My commands:

% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------
\begin{document}
\lstset{basicstyle=\small\ttfamily,xleftmargin=15pt,language=Matlab,
        commentstyle=\color{green},showstringspaces=false,stringstyle=\color{magenta}\ttfamily}

% -----------------------------------------------------------------------------
% This is the title page
\begin{frame}[t,plain]
	\titlepage
\end{frame}


% -----------------------------------------------------------------------------
% Motivation slide
\begin{frame}[t]{Motivation} % 1 folie
% \textcolor{blue}{\Large Training objective}
\only<1-1>{
\textcolor{blue}{\Large Speech Emotion Recognition}\\
	\begin{itemize}
		\itemsep15pt
% 		\item Most current work focuses on speech processing based on linguistic information,  e.g.: Skype Translator
		\item More natural human-machine interaction requires paralinguistic information such as age, gender, emotion.
		\item Emotion data are high-dimensional complex data with non-linear temporal hidden features
		\item GMM is not sufficient enoug to modelling speech emotion
% 		\item Speech Recognition / Speeker Identification / Emotion Recognition
		\only<1-1>{\begin{figure}[b]
		 \includegraphics[width=0.6\linewidth]{paraliguistic.png}
		\end{figure}}

	\end{itemize}
}

\only<2-2>{
\textcolor{blue}{\Large Deep Learning}\\
	\begin{itemize}
	    \itemsep15pt
	    \item New research area of machine learning (from shallow to deep structure)
	    \item Deep architecture for extracting complex structure and building internal representations via unsupervised learning
	    \item Widely applied in vision/audition processing, e.g. handwriting recognition (Graves, Alex, et al. 2009), traffic sign classification (Schmidhuber, et al. 2011), text translation (Google, 2014)
	\end{itemize}
}
\end{frame}

% -----------------------------------------------------------------------------
% This is the table of contents. You can insert a motivation before or after this slide.
\begin{frame}
	\ifthenelse{\equal{\lang}{ngerman}}{
		\frametitle{Table of Contents}
	}{
		\frametitle{Table of Contents}
	}
	\tableofcontents
\end{frame}

% Add an extra slide at the beginning of each section while highlighting the current section
% Use \section* to skip the slide once or comment the following to skip all overview slides.
\AtBeginSection[]
{
	\begin{frame}<beamer>
		\ifthenelse{\equal{\lang}{ngerman}}{
			\frametitle{Table of Contents}
		}{
			\frametitle{Table of Contents}
		}
% 		\frametitle{\contentsname}
		\tableofcontents[currentsection]
	\end{frame}
}

%% =========
\section{Foundations} %1 F
% -----------------------------------------------------------------------------
	\begin{frame}[t]{Foundations}
	\txtcolb{Mel Frequency Cepstral Coefficients}
			\begin{itemize}
				\item most commonly used features in speech emotion analysis 
				\item short-term power spectrum based on frame
				\item mel-scale approximate human perception
				\begin{align}
				f_{mel} = 1125~\ln~(1+f_{Hz}/700)\nonumber
				\end{align}
			\end{itemize}

		\begin{figure}
		 \includegraphics[width = 0.5\linewidth]{MelvsHz.png}
		\end{figure}
	\end{frame}
	
	
	\begin{frame}[t]{Pre-processing for MFCCs}
	\txtcolb{Steps to MFCCs}
		      \begin{enumerate}
		      \item Convert speech signal with overlapped window to frames ($20ms$ frame length, $10ms$ shifting).
		      \item Calculate power spectrum for each frame with DFT and take the logarithm value.
		      \item Apply Mel filterbank to the power spectrum, sum the power in each filter.
		      \item Decorrelation by applying Discrete Cosine Transform (DCT) to the logarithm of the filter powers.
		      \item Keep coefficients $1-20$ of DCT and discard the rest.
		      \end{enumerate}

		      \begin{figure}[htbp]
		      \includegraphics[width= \textwidth]{mfcc.png}
		      \end{figure}
	\end{frame}
	
	\begin{frame}[t]{Foundations}
	\txtcolb{Framework of Emotion Recognition}
	      \begin{itemize}
	       \item Pre-processing of emotion data to extract MFCC features
	       \item Model data distribution based on MFCCs via unsupervised learning
	       \item Classification with supervised learning
	      \end{itemize}
	      \vspace{10mm}
	      \includegraphics[width=\linewidth]{Framework.png}

	\end{frame}
% 
% 
% % 
% \subsection{Emotion Recognition Approaches}
% 
% \begin{frame}[t]{Emotion Recognition Approaches}
% 	\begin{minipage}[t]{0.48\linewidth}
% 	  \textcolor{blue}{\Large Traditional Approaches}
% 	  \begin{itemize}
% 	   \item pre-selected features
% 	   \item supervised training
% 	   \item low-level features not appropriate for claasification
% 	   \item shallow structure of classifiers
% 	  \end{itemize}
% 	\end{minipage}\hfill
% 	\begin{minipage}[t]{0.48\linewidth}
% 	\textcolor{blue}{\Large Deep Learning Approaches}
% 	  \begin{itemize}
% 	   \item learning representations from high-dim data
% 	   \item extracting appropriate features without hand-crafting
% 	   \item low-level features are used to build high-level features as network gets deeper
% 	   \item frame-based classification
% 	  \end{itemize}
% 
% 	\end{minipage}
% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conditional Restricted Boltzmann Machine} %% 
\subsection{Restricted Boltzmann Machine}
	\begin{frame}[t]{Restricted Boltzmann Machine}
	 \begin{itemize}
	  \itemsep10pt
	  \item Generative energy-based graphical model, capture data distrbution $P(\x|\boldsymbol{\theta})$
	  \item Trained in unsupervised way, only use unlabeled input sequence $\x$ for learning. 
		  \begin{itemize}
		   \item automatically extract useful features from data 
		   \item find hidden structure (distribution). 
		   \item learned features used for prediction or classification
		  \end{itemize}
	  \item Successfully applied in motion capture (Graham W. Taylor, Geoffrey E. Hinton, 2006)
% 	  \item speicifies a joint distribution over input and hidden variables, can either generating data, or with bayesian
% rule to form conditional distribution. 
	  \item non-temporal, but is potential to be extended to capture temporal information
	 \end{itemize}
	\end{frame}
	
	\begin{frame}[t]{Restricted Boltzmann Machine}
	\txtcolb{Structure}
	    \begin{figure}[t]
		\includegraphics[width=0.9\linewidth]{RBMStruct.png}
	    \end{figure}
	    \only<1-1>
	    {\begin{align}
	      &\text{visible/input layer} &\x \in \{0,1\}\nonumber\\
	      &\text{hidden layer} &\h \in \{0,1\}\nonumber\\
	      &\text{parameter set} &\btheta = \{\mathbf{W},\mathbf{b},\mathbf{c}\}\nonumber
	     \end{align}

	    }
	    \only<2->{
	      \begin{minipage}{0.48\linewidth}
	      \begin{align}
		\text{Energy Function:}~&E_{\boldsymbol{\theta}}(\x,\h) = -\mathbf{x^{T}}\mathbf{W}\mathbf{h}-\mathbf{b^{T}}\x-\mathbf{c^{T}}\mathbf{h}\nonumber\\
	      \text{Joint Distribution:}~&P^{RBM} (\x,\mathbf{h}) = \frac{1}{Z} e^{-E_{\boldsymbol{\theta}}(\x,\mathbf{h})}\nonumber\\
	      \text{Partition Function:}~ &Z = \sum_{\mathbf{x,h}} e^{-E_{\boldsymbol{\theta}}(\x,\mathbf{h})}\nonumber\\
	      \text{Free Energy:}~& \mathcal{F}(\x) = - \log \sum_h e^{-E(\mathbf{x,h})}\nonumber
	      \end{align}
	      \end{minipage}
	    }
	\end{frame}
	
	\begin{frame}[t]{Inference}
	\txtcolb{Inference}
		\begin{minipage}[t]{0.48\linewidth}

			 \begin{align}
			 &P(\x) = \sum_{\mathbf{h}} P(\mathbf{x,h})\nonumber\\ 
			 &P (\mathbf{h}) = \sum_{\x} P(\mathbf{x,h})\nonumber\\
			 &P (\h|\x)= \dfrac{P(\x,\h)}{P(\x)}\nonumber\\
			 &P(\x|\h)= \dfrac{P(\x,\h)}{P(\h)}\nonumber\\
			 &P(h_{j}=1 \mid \x)= sigmoid(\sum_{i} x_{i}W_{ij} + c_{j})\nonumber\\
			 &P(x_{i}=1 \mid \mathbf{h}) = sigmoid(\sum_{j} W_{ij} h_{j} + b_{i})\nonumber
			 \end{align}
		 \end{minipage}
	\end{frame}
 \subsection{CRBM}
	\begin{frame}[t]{Conditional RBM}
	    \only<1-2>{
	    \begin{itemize}
	     \item Consider visible units from previous time step as additional bias for current visible and hidden layer
	     \item $A$ and $B$ are weight parameter of visible (history) - visible and visible (history) - hidden connections
	     \item Visible layer is linear units with independent Gaussian noise to model real-valued data, e.g. spectral features
	    \end{itemize}
	    }
	    \only<2->{
	    \begin{figure}[t]
	    \includegraphics<2>[width=0.7\linewidth]{CRBM.png}
	    \includegraphics<3>[scale = 0.2]{CRBM.png}
	    \end{figure}
	    }
	    \only<3->{\begin{align}
	      \text{Energy Function:}~&E^{CRBM}_{\boldsymbol{\theta}} (\x, \mathbf{h})  = \left\| \frac{\x - \tilde{\mathbf{b}}}{2} \right\|^{2}
    -\tilde{\mathbf{c}}^T \mathbf{h}-\x^{T} \mathbf{W}\mathbf{h} \nonumber\\
	      &\tilde{\mathbf{b}} = \mathbf{b} + \mathbf{A}\cdot \x_{<t}\nonumber\\
	      &\tilde{\mathbf{c}} = \mathbf{c} + \mathbf{B}\cdot \x_{<t}\nonumber\\
	      &\boldsymbol{\theta} = \left \{\mathbf{W,A,B,b,c} \right\}\nonumber\\
	      \text{Free Energy:}~&\mathcal{F}(\x) = \left\| \x - \tilde{\mathbf{b}} \right\|^{2} - \log (1+e^{\tilde{\mathbf{c}}+\x\cdot \mathbf{W}})\nonumber
	    \end{align}
	    }


	\end{frame}
	
	\begin{frame}[t]{Training of Energy-based Model}
% 	      \begin{columns}[c]
% 		    \column{3in}
		    \alert<1-1>{Maximum Likelihood Estimation} $P(\x|\btheta)$\\\vspace{5mm}
		    \only<2->{
		    \textcolor{blue}{Kullback-Leibler Divergence}:
		    \begin{align}
			Q(\x)\|P(\x|\btheta) &=  \int_{-\infty}^{\infty} Q(\x)\cdot \log \dfrac{Q(\x)}{P(\x|\btheta)}\mathrm{d}\x \nonumber\\
						& = \int_{-\infty}^{\infty}Q(\x)\cdot\log Q(\x) \mathrm{d} \x - \int_{-\infty}^{\infty}Q(\x)\cdot\log P(\x|\btheta) \mathrm{d} \x \nonumber\\
						& = \left \langle \log Q(\x)\right\rangle_{Q(\x)} - \left\langle \alert<3->{\log P(\x|\btheta)}\right\rangle_{Q(\x)}\nonumber
		    \end{align}
		    $Q(\x)$, true data distribution\\\vspace{2mm}
		    $P(\x|\btheta)$, model distribution\\\vspace{2mm}
		    $\left\langle\cdot\right\rangle_{Q(\x)}$, expectation w.r.t. $Q(\x)$\\\vspace{2mm}
		    Note that KL is non-negative}
% 	      \end{columns}
	 \end{frame}

	
	\begin{frame}[t]{Training of Energy-based Model}
		
		\uncover<1-3>{
		\begin{columns}
		 \column{5in}
		      \begin{align}
				- \log P(\x|\btheta) = \mathcal{F}(\x) + \log\sum_{\x}\sum_{\h}e^{-E_{\btheta}(\x,\h)}\nonumber
		      \end{align}
% 		\column{1.5in}
% 		Free Energy
		\end{columns}
		}
		
		      \begin{columns}
			    \column{4in}
		\only<2->{  \begin{align}
				    -\frac{\partial  \log P(\x|\btheta)}{\partial \boldsymbol{\theta}} = \frac{\partial \mathcal{F}(\x)}{\partial \boldsymbol{\theta}} -
				    \sum_{\tilde{\x}} P(\tilde{\x})
					\frac{\partial \mathcal{F}(\tilde{\x})}{\partial \boldsymbol{\theta}}\nonumber
			    \end{align}
			    $\x$, input (visible) data space\\
			    $\tilde{\x}$, all possible vectors in the data space, generated by model.\vspace{5mm}
			}
		\only<3->
		{objective function by averaging log-likelihood over data:
			    \begin{align}\label{loggra}
				    - \left\langle\frac{\partial  \log P(\x|\btheta)}{\partial \boldsymbol{\theta}}\right\rangle_{\x} = \left\langle \frac{\partial \mathcal{F}(\x)}{\partial \boldsymbol{\theta}}\right\rangle_{\x} -\alert<3->{
					  \left\langle \frac{\partial \mathcal{F}(\tilde{\x})}{\partial \boldsymbol{\theta}}\right\rangle_{\tilde{\x}}\nonumber}
			    \end{align}
		}
% 		      \column{1.5in}
%       % 		\[ \xleftarrow{\hspace*{3mm}} \]
% 		      \textcolor{red}{$\leftarrow$intractable!}
		      \end{columns}

% 		\only<3->{
% 		      \begin{columns}
% 		      \column{3in}
% 			\begin{align}
% 			      - \frac{\partial  \log P(\x|\btheta)}{\partial \boldsymbol{\theta}} = \frac{\partial \mathcal{F}(\x)}{\partial \boldsymbol{\theta}} - \alert<3-3>{ \frac{1}{|\mathcal{N}|}
% 			    \sum_{\tilde{\x}\in \mathcal{N}} P(\tilde{\x})
% 				\frac{\partial \mathcal{F}(\tilde{\x})}{\partial \boldsymbol{\theta}}\nonumber}
% 			\end{align}
% 		      \column{1.5in}
% 		      sampling
% 			\end{columns}
% 		  }
	\end{frame}
	
	\begin{frame}[t]{Training of Energy-based Model}
% % 		Sampling Method: \textcolor{red}{Gibbs Sampling}
		\txtcolb{Gibbs sampling}
		\begin{columns}[c]
			  \column{1.5in}
			\uncover<1-1>{\begin{align}
			      & \x^{(1)} \sim P(\x)\nonumber \\
			      & \mathbf{h}^{(1)} \sim P(\mathbf{h}|\x^{(1)})\nonumber	
			\end{align}
			}
			\uncover<2-2>{
			\begin{align}
				  & \x^{(2)} \sim P(\x|\mathbf{h}^{(1)})\nonumber\\
				  & \mathbf{h}^{(2)} \sim P(\mathbf{h}|\x^{(2)})\nonumber
			\end{align}
			}
			\uncover<3-3>{
			\begin{align}
				& \vdots \nonumber\\
				& \x^{(k)} \sim P(\x|\mathbf{h}^{(k-1)})\nonumber	
			\end{align}
			}

			  \column{3in}
			  \framebox{\includegraphics<1>[width=\textwidth]{markov_chain.png}
				    \includegraphics<2>[width=\textwidth]{markov_chain2.png}
				    \includegraphics<3>[width=\textwidth]{markov_chaint.png}
				    }
		\end{columns}
% 		\only<2->{
% 		\begin{columns}[c]
% 			  \column{1.5in}
% 
% 			  \column{1.5in}
% 			  \framebox{\includegraphics[width=\textwidth]{markov_chain2.png}}
% 		\end{columns}
% 		}
% 		\only<3->{
% 		\begin{columns}[c]
% 			  \column{1.5in}
% 
% 			  \column{1.5in}
% 			  \framebox{\includegraphics[width=\textwidth]{markov_chaint.png}}
% 		\end{columns}
% 		}
% 		
% 	\only<4->{$t=1$, Gibbs step $\rightarrow$ \textcolor{red}{Contrastive Divergence}}
	\end{frame}
	
	\begin{frame}[t]{Contrastive Divergence}
% 	 \textcolor{blue}{Contrastive Divergence}\\
	 \begin{itemize}
	    \itemsep10pt
	    \item k=0, $P_0(\x)$ is true data distribution, independent of parameter $\btheta$
	    \item Performing k-Gibbs steps to generate $P_k(\x|\btheta)$, with $k\rightarrow \infty$ the Markov chain converges to stationary distribution:
		  \begin{align}
		      P_{\infty}(\x|\btheta) \rightarrow P(\tilde{\x}|\btheta)\nonumber
		  \end{align}\\
	    \only<2->
	    {Rewrite objective function:
		  \begin{align}
		   - \left\langle\frac{\partial  \log P(\x|\btheta)}{\partial \boldsymbol{\theta}}\right\rangle_{P_0(\x)} = \left\langle \frac{\partial \mathcal{F}(\x)}{\partial \boldsymbol{\theta}}\right\rangle_{P_0(\x)} -\alert<3->{
					  \left\langle \frac{\partial \mathcal{F}(\x)}{\partial \boldsymbol{\theta}}\right\rangle_{P_{\infty}(\x|\btheta)}\nonumber}
		  \end{align}
	    }
% 	    \item Difference between approximation and true model distribution:
% 		\begin{align}
% 		    KL(P_{k}(\x|\btheta)\|P(\x|\btheta))\nonumber
% 		\end{align}
% 	    \item CD-1 performs well in practice
	 \end{itemize}
	  
	 
	\end{frame}
	
	\begin{frame}[t]{Constrasive Divergence}
	\txtcolb{Contrastive Divergence}:
	 Perform CD-1
	    \begin{align}
	    &-\dfrac{\partial}{\partial \btheta} (P_0\|P_{\infty}^{\btheta} - P_{1}^{\btheta}\|P_{\infty}^{\btheta})\nonumber\\\vspace{3mm}
	    &=\left\langle \frac{\partial \mathcal{F}(\x)}{\partial \boldsymbol{\theta}}\right\rangle_{P_0} 
		  -\left\langle \frac{\partial \mathcal{F}(\x)}{\partial \boldsymbol{\theta}}\right\rangle_{P_{1}^{\btheta}}\nonumber
		  \uncover<1-2>{+ \alert<2-2>{\frac{\partial P_{1}^{\btheta}}{\partial \btheta} \frac{\partial (P_{1}^{\btheta}|P_{\infty}^{\btheta})}{\partial P_{1}^{\btheta}}\nonumber}}
		  \only<3->{ \cancel{+\alert<3-3>{\frac{\partial P_{1}^{\btheta}}{\partial \btheta} \frac{\partial (P_{1}^{\btheta}|P_{\infty}^{\btheta})}{\partial P_{1}^{\btheta}}\nonumber}}}
	    \end{align}
	    
	 \only<4->
	 {\txtcolb{Parameter Update}
	 \begin{align}
	  \Delta \btheta \sim \left\langle \frac{\partial \mathcal{F}(\x)}{\partial \boldsymbol{\theta}}\right\rangle_{P_0} 
		  -\left\langle \frac{\partial \mathcal{F}(\x)}{\partial \boldsymbol{\theta}}\right\rangle_{P_{1}^{\btheta}}\nonumber
	 \end{align}
	}
	\end{frame}
% 
% 
% 
% 
% 
%   
% % % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% =========

\section{Deep Neural Network}%4 folies
	\subsection{Function and Training}		
	\begin{frame}[t]{Structure and Function}
% 	\txtcolb{N-hidden layers neural network }
	  \begin{columns}[T]
	      \column{2in}
	      Hidden layer pre-activation:
	       \begin{align}
	       \mathbf{a}(\x) =\mathbf{W}^{(1)}\x + \mathbf{b}^{(1)}\nonumber\\
	       a_{j}(\x) = \sum_i w_{ji}^{(1)}x_{i} + b_{j}^{(1)}\nonumber
	       \end{align}
	       Hidden layer activation:
	       \begin{align}
		  \h = f(\mathbf{a})\nonumber
		\end{align}
	  \column{2.5in}
	  		    \tikzset{neuron/.style={circle,thick,fill=black!25,minimum size=17pt,inner sep=0pt},
			input neuron/.style={neuron, draw,thick, fill=gray!30},
			hidden neuron/.style={neuron,fill=white,draw},
			output neuron/.style={neuron,draw,thick,fill=black!20},
			hoz/.style={rotate=-90}}   %<--- for labels

		   \scalebox{0.7}
		   {
		   \begin{tikzpicture}[t,-,draw=black, node distance=\layersep,transform shape,rotate=90,width=0.2\textwidth,height=0.2\textwidth]  %<-- rotate the NN

		    % Draw the input layer nodes
		    \foreach \name / \y in {1/1,3/n}
		    \node[input neuron, hoz] (I-\name) at (0,-\name) {\color{black}$x_\y$};

		    \node[hoz] () at (0,-2) {$\dots$};
		    % Draw the hidden layer nodes
		    \foreach \name / \y in {1/1,2/2,4/\text{\tiny n-1},5/n}
		    \path[yshift=1cm] node [hidden neuron, hoz] (H-\name) at (\layersep,-\name cm) {\color{black}$h_\y$};
		    \node [output neuron, hoz] at (3.8,-5) {\color{black}$\mathbf{b}^{(2)}$};
		    \node [hidden neuron, hoz] at (1.3,-5) {\color{black}$\mathbf{b}^{(1)}$};
		    \path[yshift=1cm]
		      node[hoz] () at (\layersep,-3 cm) {$\dots$};
% 		    \path[yshift=0.5cm]
% 		      node[hoz] () at (\layersep,-5 cm) {$\dots$};


		    %%%%%%%%%% Draw the output layer nodes
		    \foreach \name / \y in {1/1,2/2,3/3,4/4}
		    \path[yshift=0.5cm] node [output neuron, hoz](O-\name) at (\layersept,-\name cm){\color{black}$y_\y$};

		    % 		      \node[output neuron,hoz] () at (\layersept,-3 cm) {$\dots$};
					  
		    \path node[hoz,right] at (1.3,1) {\color{black}$\mathbf{W}^{(1)}$};      
		    \path node[hoz,right] at (3.8,1) {\color{black}$\mathbf{W}^{(2)}$}; 
		    \path node[hoz,right] at (\layersep,-4.5 cm) {\includegraphics[width=0.2\textwidth]{activation.png}}; 
		    
		    
		    % Connect every node in the input layer with every node in the  hidden layer.
			\foreach \source in {1,3}
			    \foreach \dest in {1,2,4,5}
				\path[->] (I-\source.north) edge (H-\dest.south);
			\foreach \source in {1,2,4,5}
			  \foreach \dest in {1,2,3,4}
			      \path[->] (H-\source.north) edge (O-\dest.south);
		    \end{tikzpicture}
		    }
	  \end{columns}\vspace{5mm}
	  \begin{columns}[T]
	   \column{3in}
		Output layer activation of single hidden layer:
		 \begin{align} 
		 \hat{y}(\x) = o(\mathbf{W}^{(2)}\h^{(1)} + \mathbf{b}^{(2)} )\nonumber
		 \end{align}
		 Output layer activation of $N$ hidden layers:
		 \begin{align} 
		 \hat{y}(\x) = o(\mathbf{W}^{(N+1)}\h^{(N)} + \mathbf{b}^{(N+1)} )\nonumber
		 \end{align}
	    \column{1.5in}
	  \end{columns}

	  
% 	  \begin{minipage}{0.45\linewidth}
% 		\begin{figure}
% 		      \includegraphics{}
% 		\end{figure}
% 	  \end{minipage}

	\end{frame}
	\begin{frame}[t]{Training}
		\textcolor{blue}{\Large Empirical Risk Minimization}
		\begin{itemize}
		 \item learning algorithms\\
		 \begin{eqnarray}
			\text{arg}~\underset{\btheta}{\mathrm{min}} \frac{1}{M}\sum_m l(\hat{y}(\x^{(m)};\btheta),y^{(m)}) + \lambda \Omega(\btheta)\nonumber
			\end{eqnarray}
		 \item loss function $l(\hat{y}(\x^{(m)};\btheta),y^{(m)})$ \\
			for sigmoid activation $l(\btheta)= \sum_m \frac{1}{2} \left\| y^{(m)}-\hat{y}^{(m)}\right\| ^2$\\
		 \item regularizer $\lambda \Omega(\btheta)$
		\end{itemize}
		
		\txtcolb{Optimization}
		\begin{itemize}
		 \item Gradient calculation with Backpropagation
		 \item Stochastic/Mini-batch gradient descent
		\end{itemize}

		
		
	\end{frame}
	
	
	\subsection{Problems and Solutions}
		
	\begin{frame}[t]{Unsupervised Layerwise Pre-training}
		\begin{minipage}[h]{\linewidth}
		\txtcolb{Vanishing Gradient}
			\begin{itemize}
				\item Training time increases as network gets deeper
				\item Gradient shrink exponentially and training end up local minima
				\item Caused by random initialization of network parameters
			\end{itemize}
		\end{minipage}\vspace{5mm}
		\begin{minipage}[h]{\linewidth}
		\only<1-1>{
		\begin{figure}
		 \includegraphics[width=0.6\textwidth]{GradVanishing.png}
		\end{figure}

		}
		\visible<2->{
		\txtcolb{Unsupervised layerwise pre-training}
		\begin{itemize}
		 \itemsep8pt
		 \item Pretrain the deep network layer by layer to build a stacked auto-encoder
		 \item Each layer is trained as a single hidden layer auto-encoder by minimizing average reconstruction error:\\
		      $\mathrm{min}~l_{AE} = \sum_m \frac{1}{2}\left\|\x^{(m)}-\hat{\x}^{(m)}\right\|^2$
		 \item Fine-tuning the entire deep network with supervised training
		\end{itemize}
		}
		\end{minipage}		
	\end{frame}

% 		\begin{itemize}
% 			\item Optimization problem non-convex\\
% 			$\Rightarrow$ getting stuck in poor local minima
% 			\item Diffusion of gradients
% 			\item Large p small n problem $\Rightarrow$ overfitting
% 	
% 	\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%Noob pictures for pretraining%%%%%%%%%%%%%%%
	\begin{frame}[t]{Pre-training}
	 \includegraphics[width=0.9\linewidth]{layerwisewhole.png}
% 	    \begin{columns}
% 	     \column{4in}
% 		\begin{figure}
% 		 \subfigure{\includegraphics[width=0.4\textwidth]{layerwise1.png}}\\
% 		 \subfigure{\includegraphics[width=0.4\textwidth]{layerwise2.png}}
% 		\end{figure}
% 	     \column{4in}
% 		 \begin{figure}
% 		  \subfigure{\includegraphics[width=0.4\textwidth]{layerwise3.png}}\\
% 		  \subfigure{\includegraphics[width=0.4\textwidth]{layerwisewhole.png}}
% 		 \end{figure}
% 	      \end{columns}
	\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%

	
	\begin{frame}[t]{Regularization}
		\begin{minipage}[h]{\linewidth}
		\txtcolb{Overfitting}
			\begin{itemize}
				\item Huge amount of parameters in deep network
				\item Not enough data for training
				\item Poor generalization 
			\end{itemize}
		\end{minipage}\vspace{5mm}
		\visible<2->{
		\begin{minipage}[h]{\linewidth}
		\txtcolb{Regularization}
			\begin{itemize}
			\item Add weight penalization $\lambda \left\| \mathbf{w} \right\| _{p}$ to loss function 
			      \begin{eqnarray}
			      \text{arg}~\underset{\btheta}{\mathrm{min}} \frac{1}{M}\sum_m l(\hat{y}(\x^{(m)};\btheta),y^{(m)}) + \lambda \left\| \mathbf{w} \right\| _{p}\nonumber
			      \end{eqnarray}
			\item 	In convex optimization:
	 		\begin{eqnarray}
			\text{arg}~\underset{\btheta}{\mathrm{min}} \frac{1}{M}\sum_m l(\hat{y}(\x^{(m)};\btheta),y^{(m)}),  s.t. \left\|\mathbf{w}\right\|_p \leq C\nonumber
			\end{eqnarray}
			\end{itemize}
		\end{minipage}	
		}
	\end{frame}
	
	\begin{frame}[t]{Regularization}
	\txtcolb{P-Norm}
	\begin{eqnarray}
			      \left\| \mathbf{w} \right\| _{p} := \left( \sum_{n=1}^{n} |w_{i}|^{p} \right) ^{1/p} = \sqrt[p]{|w_1|^p +,..., +|w_n|^p}\nonumber
	\end{eqnarray}
	Widely used: L1- and L2-regularization ($p=1$ and $p=2$)
	\begin{figure}[t]
	\includegraphics<1>[width=0.7\linewidth]{contoursreg.png}
	\includegraphics<2>[width=0.7\linewidth]{l1vsl2.png}
% 	 \only<1-1>{\includegraphics[width=0.7\linewidth]{contoursreg.png}}
% 	 \only<2->{\includegraphics[width=0.7\linewidth]{l1vsl2.png}}
	\end{figure}

	\end{frame}

	
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Long Short Term Memory}
% 		\begin{frame}[t]{Recurrent Neural Network}
% % 			\begin{minipage}[t][][t]{0.48\linewidth}
% 			\textcolor{blue}{\Large Concepts of RNN}
% 				\begin{itemize}[<+->]
% % 				 \itemsep10pt
% 				 \item modelling sequential data, emotion in speech .
% 				 \item Same Structure as MLP but differs from feed-forward network, enabling
% nonlinear mapping
% 				 \only<2-2>{
% 					\begin{eqnarray}
% 						&h_{t} = \mathcal{H}(W_{xh}x_{t}+W_{hh}h_{t-1} + b_{h})\nonumber\\
% 						&y_{t} = W_{hy}h_{t} + b_{y}\nonumber
% 					\end{eqnarray}
% 				 }
% 				
% 				 \item Feedback connection between previous hidden units and current hidden units, enabling
% memory past hidden state.
% 				 \item Potentially to model arbitary dynamic system.
% 				 \item Trained with \textbf{b}ack\textbf{p}ropagation \textbf{t}hrough \textbf{t}ime (BPTT)
% 				 \note{natural extension of BP in FF network}
% 				\end{itemize}
% % 			\end{minipage}
% \vspace{5mm}
% % 		\only<1>{\begin{figure}
% % 		          \includegraphics[width=0.5\textwidth]{NeuralNetwork.png}
% % 		         \end{figure}
% % 
% % 		}
% 		\end{frame}
% 		
% 		\begin{frame}[t]{Recurrent Neural Network}
% % 			\begin{minipage}[t][][t]{0.48\linewidth}
% 			\textcolor{blue}{\Large Concepts of RNN}
% 			\begin{minipage}[t]{0.48\linewidth}
% 				\only<1>{
% 				\begin{figure}[t]
% 				\includegraphics[width=0.8\textwidth]{RNNunit.png}
% 				\end{figure}
% 				}
% 			\end{minipage}\hfill
% 			\begin{minipage}[t]{0.48\linewidth}
% 				\only<1>{
% 				\begin{figure}[t]
% 				\includegraphics[width=\textwidth]{RNNStruct.png}
% 				\end{figure}
% 				}
% 			\end{minipage}
% 
% 		\end{frame}
    
    

		\begin{frame}[t]{Long short term memory}
		    \only<1-2>
		    {\textcolor{blue}{\Large Problems with RNN}
			\begin{itemize}%[<+->]
			 \item gradient vanishing during backpropagation as time steps increases (>100)
			 \item difficult to capture long-time dependency (which is required in emotion recognition)
			\end{itemize}
		    }
		    \only<2-2>
			{\vspace{5mm}S. Hochreiter and J. Schmidhuber, Lovol. 9, pp. 1735-1780, 1997.\\
			}
		    \only<3->{
		    \includegraphics<3>[scale=0.35]{LSTMstruct.png}
		    \begin{columns}[T]
			\column{1.5in}
			 \includegraphics<4>[width=\linewidth]{LSTMstruct.png}
			 \only<4->{
			 \column{1.5in}
			 \begin{align}
				  i_{t} &= \sigma (W_{xi}x_{t} + W_{hi}h_{t-1} + W_{ci}c_{t-1} +b_{i})\nonumber\\
				  f_{t} &= \sigma (W_{xf}x_{t} + W_{hf}h_{t-1} + W_{cf}c_{t-1} +b_{f})\nonumber\\
				  c_{t} &= f_{t}c_{t-1} + i_{t}\mathrm{tanh}(W_{xc}x_{t} + W_{hc}h_{t-1} + b_{c})\nonumber\\
				  o_{t} &= \sigma (W_{xo}x_{t} + W_{ho}h_{t-1} + W_{co}c_{t} +b_{o})\nonumber\\
				  h_{t} &= o_{t}\mathrm{tanh} (c_{t})\nonumber
			 \end{align}
			 } 
		      \end{columns}
		     } 
		\end{frame}
		
		\begin{frame}[t]{Long short term memory}
		\textcolor{blue}{\Large Features in LSTM}
		      \begin{itemize}
			\item gates are trained to learn when it shoud be open/closed. 
			\item Constant Error Carousel
			\item preserve long-time dependency by maintaining gradient over time. 
		      \end{itemize}
		      
		      \begin{figure}
			\includegraphics[width=0.8\textwidth]{LSTMGra.png}
		      \end{figure}


		\end{frame}
		
		
% % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
	\begin{frame}[t]{Experiment Setup}
	  \txtcolb{EmoDB Database}
	  \begin{figure}
		    \begin{tabular}{l|*{4}{c}|c}
			  & Joy & Neutral & Sadness & Anger & Total\\
			\hline
			No. of sentences &71	&79	&62	&127 & 339\\
			\hline
		    Percent (\%) &21 & 23.2 & 18.3& 37.5 & 100
		    \end{tabular}
	  \end{figure}
	  \txtcolb{Data Structure}
	  \begin{figure}
	   \includegraphics[scale=0.3]{DataStruct.png}
	  \end{figure}
	\end{frame}

	
	\begin{frame}[t]{Experiment Setup}
		\begin{minipage}[t]{\linewidth}
		\begin{itemize}[<+->]
		  \item<only@1> CRBM-DNN 
		  \item<only@2> CRBM-LSTM
		  \item<only@3> LSTM
		  \item<only@4> LSTM with rectifier units
		\end{itemize}
		\end{minipage}\hspace{5mm}
		
		\begin{minipage}[t]{\linewidth}
		    \begin{figure}[b]
		    \only<1>{\includegraphics[width=\linewidth]{CRBMDNN.png}}
		    \only<2>{\includegraphics[width=.9\linewidth]{CRBMLSTM.png}}
		    \only<3>{\includegraphics[width=.8\linewidth]{LSTMpure.png}}
		    \only<4>{\includegraphics[width=\linewidth]{LSTM.png}}
		  \end{figure}
		\end{minipage}
	\end{frame}
	
	\begin{frame}[t]{Result}
	      \only<1>
	      {
	      \begin{table}[htbp]\centering
	      \centering
	      Confusion matrix of CRBM-DNN result.\\
	      \vspace{10mm}
	      \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}} cl*{4}c @{}}
		  \toprule
		  & \multicolumn{5}{c}{\textit{{Classfied}}} \\[1ex]
	      %     \midrule
		  \multirow{5}{*}{\textit{True}}
		  & & Joy & Neutral & Sadness & Anger \\
	      %     \cmidrule{2-12}
		  & Joy             &\textcolor{red}{57.7\%} &1.4\%   		  & 0.0\%		& 40.8\%\\
		  & Neutral         & 17.7\%			&\textcolor{red}{54.4\%} &25.3\%   	&2.5\%     \\
	      %     \rot{\rlap{~\textit{{True}}}}
		  & Sadness         &1.6\%			&27.9\%   		  &\textcolor{red}{70.5\%}   &0.0\%    \\
		  & Anger           & 39.4\%			&1.6\%  		  &0.0\%   	&\textcolor{red}{59.1\%}    \\
		  \midrule
		  & \multicolumn{5}{c}{recognition rate:59.76\%}\\
		  \bottomrule
	      %     \cmidrule[1pt]{2-12}
		\end{tabular*}
	      \label{tab:CRBMDNN}
	      \end{table}
	      }
	      \only<2>
	      {\begin{table}[htbp]\centering
		\centering
		Confusion matrix of CRBM-LSTM result.\\
		\vspace{10mm}
		      \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}} cl*{4}c @{}}
			  \toprule
			  & \multicolumn{5}{c}{\textit{{Classfied}}} \\[1ex]
		      %     \midrule
			  \multirow{5}{*}{\textit{True}}
			  & & Joy & Neutral & Sadness & Anger \\
		      %     \cmidrule{2-12}
			  & Joy             &\textcolor{red}{11.3\%} &9.9\%   		  &   2.8\%	&    76.1\%\\
			  & Neutral         & 0.0\%			&\textcolor{red}{72.2\%} &17.7\%   	&10.1\%     \\
		      %     \rot{\rlap{~\textit{{True}}}}
			  & Sadness         &0.0\%			&4.8\%   		  &\textcolor{red}{88.7\%}   &6.5\%    \\
			  & Anger           & 0.8\%			&1.6\%  		  &0.0\%   	&\textcolor{red}{97.6\%}    \\
			  \midrule
			  & \multicolumn{5}{c}{recognition rate: 71.98\%}\\
			  \bottomrule
		      %     \cmidrule[1pt]{2-12}
			\end{tabular*}
		\label{tab:CRBMLSTM}
		\end{table}
	      }
	      \only<3>
	      {
	      \begin{table}[htbp]\centering
	      \centering
	      Confusion matrix of pure LSTM result.\\
	      \vspace{10mm}
		      \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}} cl*{4}c @{}}
			\toprule
			& \multicolumn{5}{c}{\textit{{Classfied}}} \\[1ex]
		    %     \midrule
			\multirow{5}{*}{\textit{True}}
			& & Joy & Neutral & Sadness & Anger \\
		    %     \cmidrule{2-12}
			& Joy             &\textcolor{red}{66.2\%} &4.2\%   		  &   0.0\%	&    29.6\%\\
			& Neutral         &6.3\%			&\textcolor{red}{79.7\%} &10.2\%   	&3.8\%     \\
		    %     \rot{\rlap{~\textit{{True}}}}
			& Sadness         &0.0\%			&19.7\%   		  &\textcolor{red}{80.3\%}   &0.0\%    \\
			& Anger           & 12.6\%			&0.8\%  		  &0.0\%   	&\textcolor{red}{86.6\%}    \\
			\midrule
			& \multicolumn{5}{c}{recognition rate: 81.59\%}\\
			\bottomrule
		    %     \cmidrule[1pt]{2-12}
		      \end{tabular*}
	      \label{tab:pureLSTM}
	      \end{table}

	      }
	      \only<4>
	      {
	      \begin{table}[htbp]\centering
	      \centering
	      Confusion matrix of LSTM-Rectifier result.\\
	      \vspace{10mm}
		      \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}} cl*{4}c @{}}
			  \toprule
			  & \multicolumn{5}{c}{\textit{{Classfied}}} \\[1ex]
		      %     \midrule
			  \multirow{5}{*}{\textit{True}}
			  & & Joy & Neutral & Sadness & Anger \\
		      %     \cmidrule{2-12}
			  & Joy             &\textcolor{red}{57.7\%} &7.0\%   		  &   0.0\%	&    35.2\%\\
			  & Neutral         &6.3\%			&\textcolor{red}{86.1\%} &6.3\%   	&1.3\%     \\
		      %     \rot{\rlap{~\textit{{True}}}}
			  & Sadness         &0.0\%			&6.6\%   		  &\textcolor{red}{93.4\%}   &0.0\%    \\
			  & Anger           & 8.7\%			&0.0\%  		  &0.0\%   	&\textcolor{red}{91.3\%}    \\
			  \midrule
			  & \multicolumn{5}{c}{recognition rate: 83.43\%}\\
			  \bottomrule
		      %     \cmidrule[1pt]{2-12}
			\end{tabular*}
	      \label{tab:LSTMRec}
	      \end{table}
	      }
	\end{frame}


% 
\section{Conclusion and Outlook}
      \begin{frame}[t]{Conclusion}
	  \begin{itemize}
	  \itemsep10pt
	  \item<1-> Capturing long-term dependencies is necessary for extracting speech emotion
	  \item<2-> CRBM-DNN is inappropriate for modelling long-term dependencies (ER: $40.24\%$)
	  \item<2-> LSTM is good at modelling long time dependencies
	  \item<3-> Frame-based classification can also reach good result
		\only<3->
		{\vspace{5mm}
		\begin{itemize}
		 \itemsep10pt
		 \item CRBM-LSTM $71.98\%$
		 \item LSTM $81.59\%$
		 \item LSTM with rectifier layers $83.43\%$
		\end{itemize}
		}
	  \end{itemize}
	  
	  \only<2-2>{
		    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}} cl*{4}c @{}}
		  %     & \multicolumn{5}{c}{\textit{\tiny{Classfied}}} \\[1ex]
		      \toprule
		      &Model & Temporal Dependency & Memory & Generaltive \\
		      \midrule
		  %     \cmidrule{2-12}
		      & DNN		& -	&-	& -	\\
		      & RBM             	& - 	& - 	&  \OK   \\
		      & CRBM         	& \OK 	& 2-5 	& \OK     \\
		      & AE         	& - 	& -  	&  - 	   \\
		      & RNN           	& \OK 	& 1-100	&  -	  \\
		      & LSTM		& \OK	& 1-1000& -	\\
		      \bottomrule
		  %     \cmidrule[1pt]{2-12}
		    \end{tabular*}
		    }
      \end{frame}

%       
      \begin{frame}[t]{Outlook}
	  \begin{itemize}
	   \itemsep15pt
	   \item Stacking CRBM to form deeper structure
	   \item Train CRBM with more/larger database 
	   \item Second order optimization to speed up learning process
	   \item Bi-directional LSTM, capturing future dependencies
	  \end{itemize}

      \end{frame}

      \begin{frame}{End}
	\begin{minipage}[c]{\linewidth}
	\centering
	\textbf{\Huge Thank You!}
	\end{minipage}
      \end{frame}



 \end{document}
